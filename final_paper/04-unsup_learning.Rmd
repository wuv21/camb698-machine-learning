# Unsupervised Learning {#unsup_learn}
## Introduction to unsupervised learning
Unsupervised machine learning is arguably a more difficult task as the classifications are not known *a priori*. These methods use clustering to discover ways that the data can be grouped. Techniques mentioned above such as PCA, PCoA, and t-SNE can be useful in viewing the data in a lower dimension and can be a good starting point for unsupervised learning. This paper will cover 1) hierarchical clustering, 2) divisive clustering, and 3) hidden Markov models. As with the supervised learning section, some of these methods will be only be described while others will contain accompanying R code and graphics.

## Hierarchical agglomerative clustering 
This type of unsupervised learning can be seen from recent papers that utilize heatmaps and have dendrograms drawn on either or both axes. These dendrograms are the final product of hierarchical clustering. Distance measurements between each data point are used to cluster the data points, which can then be portrayed as a binary tree (each branch point separates into two children). In a hierarchical clustering dendrogram, the similarities between two points can be assessed by looking at the height of the first shared node (analogous to looking at the most recent common ancestor in a phylogeny). Hierarchical clustering can provide insight on any underlying structure and can be useful when accompanied with heatmaps to show clusters in a quantitative and qualitative manner.

## Hierarchical divisive clustering
Divisive clustering is a type of hierarchical clustering that differs from agglomerative clustering by its approach in dividing the data. Agglomerative clustering, as its name suggests, starts with the data points as individual parts and starts grouping those together to create the tree (as if drawing the tree from the branches to the root). Divisive takes the opposite approach and starts with all the data points in one group and divides that group to create the tree (as if drawing the tree from root to the branches). The advantage of divisive clustering is that it takes into account how the data looks at a global perspective, whereas agglomerative clustering loses that perspective when starting with each data point as an individual.

Two of the methods to divide the first group for divisive clustering include k-means and partitioning across medoids (PAM). While there are significant differences between the two methods, the idea is similar in which clusters are created based on the proximity to the closest centroid (mean of a specific cluster of data points) or to the medoid (a data point that is selected as the center of a cluster). The difficulty for either method comes to choosing the value for $k$, which is the number of clusters that the scientist must assign *a priori*. There are multiple techniques for deciding $k$, such as the silhouette method which is shown in the below graphs.

## Hidden Markov models (HMM)
A Markov model is a model that describes a system where state transitions are random and are independent of past events. For example, the below figure shows a two state Markov model. From state A, there is a 30% chance of going back to state A or a 70% chance of going to state B regardless of any previous transitions. This simple model can be extended to multiple states with more transition weights. In HMMs, the overall idea is the same but there can also be hidden states and weights. HMMs are commonly used when studying nucleotide or amino acid sequences. Examples include sequence alignment tools as well as the HMMER program, which can take in an amino acid sequence and output proteins that are related to your sequence of interest. Furthermore, HMMs have been used for species-level and even strain-level identification for microbiome studies via the QIIME program, developed here at Penn.

```{r, fig.cap="Simple HMM example", out.width="35%", fig.align="center", auto_pdf=TRUE, echo=FALSE}
knitr::include_graphics("fig/mm_fig.png")
```


# Background {#bg}
Data processing and analysis are fundamental aspects of conducting scientific research, from the initial raw data to visualizing the results. With the recent advances of high throughput technologies (i.e. the many “-omics”, multiparametric flow cytometry, etc.), the accompanying data is often high-dimensional and difficult for manual efforts to analyze. The development of machine learning methods aims to help with this problem, thus helping to make sense of the data to draw meaningful and accurate conclusions.

The purpose of this paper is to introduce and explore some of the different machine learning methods that are commonly used in the biological sciences. To help demonstrate the methods and to maintain context across all of the methods, a single dataset (will be referred to as the PAP dataset) was provided by the PennCHOP Microbiome Program (courtesy of Dr. Kyle Bittinger). Mice were purchased from vendors with the purpose of assessing whether the mice have different phenotypes from different vendors. The fecal microbiota was sequenced from the mice, resulting in a distance matrix (how distant each mouse’s microbiome was from the other mice sampled). Additionally, metabolites in stool and in the cecum were assessed for each mouse. This high-dimensional dataset is representative of datasets that are seen with microbiome research.

Machine learning methods can fall under two main branches – supervised versus unsupervised learning. The former branch consists of methods where the concept of the output is already known. Techniques like regression and classification methods strive to produce an output (which vendor the mice are from) from an input (some or all of the variables such as the distances, the relative abundances, etc.). The latter branch contains methods where the output is not exactly known. Different clustering models attempt to use the input to find if the data can be clustered into unique groups.

While this paper is primarily focused on machine learning techniques, a concept from statistical analysis is critical for understanding the benefits and drawbacks of certain techniques. This concept, known as the *bias-variance tradeoff*, captures the relationship between approximation bias, estimation variance, and the ability to accurately predict a value [@shalizi]. Approximation bias can be loosely defined as the inherent bias in using a particular prediction function to estimate a value. The estimation variance can be defined as how spread out are the estimates. The ability to accurately predict a value is negatively affected by both the approximation bias and the estimation variance [@shalizi].

As such, the bias-variance tradeoff explores this relationship between approximation bias and estimation variance. Since reducing the approximation bias will increase the estimation variance, it may appear to be a futile attempt to maximize the ability to predict a value. The relationship, as Shalizi writes, is not “one-for-one” [@shalizi] – whereupon it is possible to reduce the error of predictions by introducing a little bias to reduce the variance. This concept will be a common motif in many of the methods in this paper.
